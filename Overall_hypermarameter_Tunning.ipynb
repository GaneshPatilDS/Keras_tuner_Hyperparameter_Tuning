{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4135f1-72d5-437b-9556-43152228740d",
   "metadata": {},
   "source": [
    "The code you provided is already quite flexible and generalized for hyperparameter tuning of neural network architectures using Keras Tuner. It allows you to specify the number of layers, units per layer, activation functions, and the optimizer to be used. However, if you want to make it even more reusable and cleaner, you can encapsulate it in a function and add more comments for clarity. Here's a slightly refined version:\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "```python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import kerastuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Counter to keep track of layer creation\n",
    "    counter = 0\n",
    "\n",
    "    # Loop to add layers based on hyperparameters\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=10)):\n",
    "        units = hp.Int('units_' + str(i), min_value=8, max_value=128, step=8)\n",
    "        activation = hp.Choice('activation_' + str(i), values=['relu', 'tanh'])\n",
    "\n",
    "        if counter == 0:\n",
    "            model.add(Dense(units, activation=activation, input_dim=8))\n",
    "        else:\n",
    "            model.add(Dense(units, activation=activation))\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define loss, optimizer, and metrics\n",
    "    LOSS_FUNCTION = \"binary_crossentropy\"\n",
    "    OPTIMIZER = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adadelta'])\n",
    "    METRICS = [\"accuracy\"]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=LOSS_FUNCTION, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a Keras Tuner RandomSearch object\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=5,\n",
    "                        directory='mydir')\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0].values\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "```\n",
    "\n",
    "In this code, I've encapsulated the model-building process into the `build_model` function, and I've made variable names a bit more descriptive. This version is more readable and easier to adapt for various problem statements. You would only need to modify the input dimension (if necessary) and perhaps the loss function or optimizer based on the specific problem you're working on.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa353c-07c8-4b73-bc1e-d9a75296f328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8608f1a-6668-4087-b708-ced9248b809b",
   "metadata": {},
   "source": [
    "--------------------------------------------------------# EXPLANATION #---------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c31654-d592-4cf4-8985-641749bd457f",
   "metadata": {},
   "source": [
    "Certainly! This code is a Python script that uses the Keras Tuner library to perform a hyperparameter search for a neural network model. Let's break it down step by step in simple language:\n",
    "\n",
    "1. **Importing Libraries:**\n",
    "   - The script starts by importing the necessary libraries. These include TensorFlow and its submodules for building the neural network, as well as the Keras Tuner library for hyperparameter tuning.\n",
    "\n",
    "2. **Defining the Model (build_model function):**\n",
    "   - A function named `build_model` is defined. This function takes one argument, `hp`, which is an instance of HyperParameters provided by Keras Tuner.\n",
    "   - Inside this function, a neural network model is created step by step. It starts as an empty model (`Sequential`).\n",
    "   - There's a loop that adds layers to the model based on hyperparameters. Specifically:\n",
    "     - It adds a variable number of dense (fully connected) layers to the model. The number of layers is determined by the hyperparameter `num_layers`, which can be set between 1 and 10.\n",
    "     - Each dense layer has a certain number of units (neurons), which is also a hyperparameter (`units`). The number of units can range from 8 to 128 with a step size of 8.\n",
    "     - The activation function for each dense layer can be either 'relu' or 'tanh', and this is another hyperparameter (`activation`).\n",
    "   - The first layer in the loop also specifies an input dimension of 8, assuming your data has 8 features.\n",
    "\n",
    "3. **Adding the Output Layer:**\n",
    "   - After the loop for adding layers, there's an additional dense layer with 1 neuron and a sigmoid activation function. This is a common setup for binary classification problems.\n",
    "\n",
    "4. **Compiling the Model:**\n",
    "   - The code specifies the loss function (`binary_crossentropy`), optimizer (`adam`, `sgd`, `rmsprop`, or `adadelta`), and metrics (`accuracy`) for training the model.\n",
    "   - These settings are crucial for how the model learns from data.\n",
    "\n",
    "5. **Creating a Keras Tuner Object:**\n",
    "   - A Keras Tuner object (`tuner`) is created using `kt.RandomSearch`. This object is responsible for conducting the hyperparameter search.\n",
    "   - The goal of the search is to maximize the validation accuracy (`objective='val_accuracy'`).\n",
    "   - You specify that you want to try up to 5 different sets of hyperparameters (`max_trials=5`).\n",
    "   - The search results will be saved in a directory called 'mydir'.\n",
    "\n",
    "6. **Performing the Hyperparameter Search:**\n",
    "   - The `tuner.search()` method is called, and it performs the hyperparameter search.\n",
    "   - It trains different versions of your model with various hyperparameter settings on the training data (`X_train` and `y_train`) and evaluates them on the validation data (`X_test` and `y_test`) for 5 training epochs.\n",
    "\n",
    "7. **Getting the Best Hyperparameters and Model:**\n",
    "   - After the search is complete, you can retrieve the best set of hyperparameters using `tuner.get_best_hyperparameters()[0].values`. This gives you the values of the hyperparameters that resulted in the best validation accuracy.\n",
    "   - You can also obtain the best model found during the search using `tuner.get_best_models(num_models=1)[0]`.\n",
    "\n",
    "In summary, this code automates the process of finding the best hyperparameters for training a neural network model. It uses Keras Tuner to search for the most suitable combination of layer architecture, neuron counts, activation functions, optimizer, and more, with the goal of achieving the highest validation accuracy. The best hyperparameters and model are then available for further use.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c223ab1-94eb-4d42-b7d4-067f29051ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
